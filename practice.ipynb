{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b46d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\anaconda3\\envs\\langgraph\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3242b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"medical.pdf\" \n",
    "CHROMA_DIR = os.getenv(\"CHROMA_DIR\", \"./chroma_store\")\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d1e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ingest] Loading PDF...\n",
      "[Ingest] Loaded 637 pages. Splitting into chunks...\n",
      "[Ingest] Total chunks: 4201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_8396\\2326865743.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ingest] Creating Chroma vector database...\n",
      "[Ingest] ✅ Ingestion complete. Data stored in: ./chroma_store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_8396\\2326865743.py:18: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "def ingest_pdf():\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        raise FileNotFoundError(f\"{PDF_PATH} not found. Place your medical PDF in the project folder.\")\n",
    "\n",
    "    print(\"[Ingest] Loading PDF...\")\n",
    "    loader = PyPDFLoader(PDF_PATH)\n",
    "    docs = loader.load()\n",
    "\n",
    "    print(f\"[Ingest] Loaded {len(docs)} pages. Splitting into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    print(f\"[Ingest] Total chunks: {len(chunks)}\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "    print(\"[Ingest] Creating Chroma vector database...\")\n",
    "    vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=CHROMA_DIR)\n",
    "    vectordb.persist()\n",
    "    print(f\"[Ingest] ✅ Ingestion complete. Data stored in: {CHROMA_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ingest_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1e7920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from typing import TypedDict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af459581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    plan: dict\n",
    "    retrieved_docs: list\n",
    "    answer: str\n",
    "    reflection: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7241e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_call(system: str, user: str) -> str:\n",
    "    client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    model = \"llama-3.3-70b-versatile\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": system},\n",
    "                  {\"role\": \"user\", \"content\": user}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a808099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieval Setup ---\n",
    "def get_retriever():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=os.getenv(\"EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    vectordb = Chroma(persist_directory=os.getenv(\"CHROMA_DIR\", \"./chroma_store\"), embedding_function=embeddings)\n",
    "    return vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0b81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Nodes ---\n",
    "def plan_node(state: AgentState) -> AgentState:\n",
    "    q = state[\"question\"]\n",
    "    plan = {\"need_retrieval\": len(q.split()) > 2, \"query\": q}\n",
    "    print(f\"[PLAN] {plan}\")\n",
    "    state[\"plan\"] = plan\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d2905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    if not state[\"plan\"][\"need_retrieval\"]:\n",
    "        print(\"[RETRIEVE] Skipped retrieval\")\n",
    "        state[\"retrieved_docs\"] = []\n",
    "        return state\n",
    "\n",
    "    retriever = get_retriever()\n",
    "    docs = retriever.invoke(state[\"plan\"][\"query\"])\n",
    "    state[\"retrieved_docs\"] = [{\"content\": d.page_content[:300]} for d in docs]\n",
    "    print(f\"[RETRIEVE] Retrieved {len(docs)} docs\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a5bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_node(state: AgentState) -> AgentState:\n",
    "    context = \"\\n\".join([d[\"content\"] for d in state.get(\"retrieved_docs\", [])])\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    system = \"You are a helpful medical assistant. Answer clearly using only provided context.\"\n",
    "    user = f\"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "    answer = groq_call(system, user)\n",
    "    print(f\"[ANSWER] {answer[:120]}...\")\n",
    "    state[\"answer\"] = answer\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af287f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_node(state: AgentState) -> AgentState:\n",
    "    question, answer = state[\"question\"], state[\"answer\"]\n",
    "    system = \"You are a strict evaluator. Score answer relevance (0–1).\"\n",
    "    user = f\"Question: {question}\\nAnswer: {answer}\\nReturn JSON: {{'relevance':score,'comment':text}}\"\n",
    "    review = groq_call(system, user)\n",
    "    try:\n",
    "        reflection = json.loads(review)\n",
    "    except:\n",
    "        reflection = {\"relevance\": 0.7, \"comment\": review}\n",
    "    print(f\"[REFLECT] {reflection}\")\n",
    "    state[\"reflection\"] = reflection\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LangGraph Build ---\n",
    "def build_agent():\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"plan\", plan_node)\n",
    "    graph.add_node(\"retrieve\", retrieve_node)\n",
    "    graph.add_node(\"answer\", answer_node)\n",
    "    graph.add_node(\"reflect\", reflect_node)\n",
    "\n",
    "    graph.set_entry_point(\"plan\")\n",
    "    graph.add_edge(\"plan\", \"retrieve\")\n",
    "    graph.add_edge(\"retrieve\", \"answer\")\n",
    "    graph.add_edge(\"answer\", \"reflect\")\n",
    "    graph.add_edge(\"reflect\", END)\n",
    "\n",
    "    return graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7714b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(question: str) -> AgentState:\n",
    "    app = build_agent()\n",
    "    init: AgentState = {\"question\": question, \"plan\": {}, \"retrieved_docs\": [], \"answer\": \"\", \"reflection\": {}}\n",
    "    final = app.invoke(init)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ecf476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "  +------+     \n",
      "  | plan |     \n",
      "  +------+     \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "+----------+   \n",
      "| retrieve |   \n",
      "+----------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "  +--------+   \n",
      "  | answer |   \n",
      "  +--------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | reflect |   \n",
      " +---------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n",
      "[PLAN] {'need_retrieval': True, 'query': 'What are the symptoms of diabetes?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_8396\\1227339322.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=os.getenv(\"CHROMA_DIR\", \"./chroma_store\"), embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RETRIEVE] Retrieved 3 docs\n",
      "[ANSWER] The symptoms of diabetes include: \n",
      "1. High blood sugar (hyperglycemia)\n",
      "2. Neurological problems, such as:\n",
      "   - Weakness\n",
      "...\n",
      "[REFLECT] {'relevance': 0.7, 'comment': \"{'relevance':0.6,'comment':'The answer provides some symptoms related to diabetes, but it primarily focuses on neurological problems, which are more commonly associated with long-term complications of diabetes rather than the initial symptoms. Typical initial symptoms of diabetes include increased thirst, frequent urination, fatigue, and unexplained weight loss, which are not mentioned in the answer.'}\"}\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "The symptoms of diabetes include: \n",
      "1. High blood sugar (hyperglycemia)\n",
      "2. Neurological problems, such as:\n",
      "   - Weakness\n",
      "   - Lack of coordination\n",
      "   - Blurred vision\n",
      "   - Loss of fine motor skills\n",
      "   - Loss of the sense of taste\n",
      "   - Ringing in the ears\n",
      "   - Loss of bladder control\n",
      "\n",
      "=== REFLECTION ===\n",
      "{'relevance': 0.7, 'comment': \"{'relevance':0.6,'comment':'The answer provides some symptoms related to diabetes, but it primarily focuses on neurological problems, which are more commonly associated with long-term complications of diabetes rather than the initial symptoms. Typical initial symptoms of diabetes include increased thirst, frequent urination, fatigue, and unexplained weight loss, which are not mentioned in the answer.'}\"}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = build_agent()\n",
    "    print(app.get_graph().draw_ascii())\n",
    "    result = run_agent(\"What are the symptoms of diabetes?\")\n",
    "    print(\"\\n=== FINAL ANSWER ===\")\n",
    "    print(result[\"answer\"])\n",
    "    print(\"\\n=== REFLECTION ===\")\n",
    "    print(result[\"reflection\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e19b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
